package main

import (
	"encoding/json"
	"fmt"
	"log"
	"os"
	"path/filepath"
	"runtime"
	"time"

	"github.com/khambampati-subhash/govecdb/index"
)

// InternalBenchmarkSuite provides comprehensive performance testing
type InternalBenchmarkSuite struct {
	// Configuration
	config *BenchmarkConfig

	// Results storage
	results []*BenchmarkResult
}

// BenchmarkConfig configures the benchmark suite
type BenchmarkConfig struct {
	// Test datasets
	DatasetSizes []int // [1000, 5000, 10000, 50000]
	Dimensions   []int // [128, 256, 512, 1024]
	QuerySizes   []int // [100, 500, 1000]

	// Index configurations
	IndexTypes  []index.IndexType // Test different index types
	MetricTypes []index.DistanceMetric

	// Performance parameters
	NumRuns    int // Number of runs per test
	WarmupRuns int // Warmup runs to ignore
	TimeoutSec int // Timeout per test

	// Memory profiling
	EnableMemoryProfiling bool
	EnableCPUProfiling    bool
	ProfileOutputDir      string

	// Comparison with external systems
	EnableExternalComparison bool
	ExternalSystems          []string // ["chromadb", "qdrant", "weaviate"]
}

// BenchmarkResult stores the results of a single benchmark
type BenchmarkResult struct {
	// Test identification
	TestName    string    `json:"test_name"`
	IndexType   string    `json:"index_type"`
	DatasetSize int       `json:"dataset_size"`
	Dimension   int       `json:"dimension"`
	QuerySize   int       `json:"query_size"`
	Metric      string    `json:"metric"`
	Timestamp   time.Time `json:"timestamp"`

	// Performance metrics
	InsertRate       float64 `json:"insert_rate"`       // vectors/sec
	SearchLatency    float64 `json:"search_latency"`    // milliseconds
	SearchThroughput float64 `json:"search_throughput"` // queries/sec
	BatchInsertRate  float64 `json:"batch_insert_rate"` // vectors/sec

	// Memory metrics
	MemoryUsed      int64 `json:"memory_used"`      // bytes
	MemoryAllocated int64 `json:"memory_allocated"` // bytes
	GCPauses        int64 `json:"gc_pauses"`        // microseconds

	// Accuracy metrics
	Recall10             float64 `json:"recall_10"`  // recall@10
	Recall100            float64 `json:"recall_100"` // recall@100
	MeanAveragePrecision float64 `json:"map"`        // MAP

	// Resource utilization
	CPUUsage  float64 `json:"cpu_usage"`  // percentage
	DiskIO    int64   `json:"disk_io"`    // bytes
	NetworkIO int64   `json:"network_io"` // bytes

	// Error metrics
	ErrorRate   float64 `json:"error_rate"`   // percentage
	TimeoutRate float64 `json:"timeout_rate"` // percentage

	// Comparison with ground truth
	GroundTruthLatency float64 `json:"ground_truth_latency"` // ms
	SpeedupFactor      float64 `json:"speedup_factor"`       // x times faster than brute force
}

// NewInternalBenchmarkSuite creates a new benchmark suite
func NewInternalBenchmarkSuite(config *BenchmarkConfig) *InternalBenchmarkSuite {
	if config == nil {
		config = &BenchmarkConfig{
			DatasetSizes:     []int{1000, 5000, 10000},
			Dimensions:       []int{128, 256, 512},
			QuerySizes:       []int{100},
			IndexTypes:       []index.IndexType{index.IndexTypeHNSW},
			MetricTypes:      []index.DistanceMetric{index.Cosine, index.Euclidean},
			NumRuns:          5,
			WarmupRuns:       2,
			TimeoutSec:       300,
			ProfileOutputDir: "./benchmark-profiles",
		}
	}

	return &InternalBenchmarkSuite{
		config:  config,
		results: make([]*BenchmarkResult, 0),
	}
}

// RunComprehensiveBenchmarks executes all benchmark tests
func (suite *InternalBenchmarkSuite) RunComprehensiveBenchmarks() error {
	fmt.Println("üöÄ Starting GoVecDB Comprehensive Benchmark Suite")
	fmt.Println("=================================================")

	// Create output directory
	if err := os.MkdirAll(suite.config.ProfileOutputDir, 0755); err != nil {
		return fmt.Errorf("failed to create profile directory: %w", err)
	}

	totalTests := len(suite.config.DatasetSizes) * len(suite.config.Dimensions) *
		len(suite.config.IndexTypes) * len(suite.config.MetricTypes)

	fmt.Printf("üìä Running %d test combinations\n", totalTests)
	fmt.Printf("üîÑ %d runs per test (%d warmup runs)\n", suite.config.NumRuns, suite.config.WarmupRuns)

	testCount := 0

	// Run benchmarks for each configuration
	for _, datasetSize := range suite.config.DatasetSizes {
		for _, dimension := range suite.config.Dimensions {
			for _, indexType := range suite.config.IndexTypes {
				for _, metric := range suite.config.MetricTypes {
					testCount++
					fmt.Printf("\nüß™ Test %d/%d: %s Index, %d vectors (%dD), %s metric\n",
						testCount, totalTests,
						indexTypeToString(indexType), datasetSize, dimension, metricToString(metric))

					result, err := suite.runSingleBenchmark(indexType, datasetSize, dimension, metric)
					if err != nil {
						fmt.Printf("‚ùå Test failed: %v\n", err)
						continue
					}

					suite.results = append(suite.results, result)
					suite.printTestResult(result)
				}
			}
		}
	}

	// Generate comprehensive report
	if err := suite.generateReport(); err != nil {
		return fmt.Errorf("failed to generate report: %w", err)
	}

	fmt.Println("\n‚úÖ Benchmark suite completed!")
	return nil
}

// runSingleBenchmark executes a single benchmark configuration
func (suite *InternalBenchmarkSuite) runSingleBenchmark(indexType index.IndexType, datasetSize, dimension int, metric index.DistanceMetric) (*BenchmarkResult, error) {
	// Generate test data
	testVectors := suite.generateTestVectors(datasetSize, dimension)
	queryVectors := suite.generateTestVectors(100, dimension) // Fixed query set size

	// Create index configuration
	config := &index.Config{
		Dimension:      dimension,
		Metric:         metric,
		M:              16,
		EfConstruction: 200,
		MaxLayer:       16,
		Seed:           42,
		ThreadSafe:     true,
	}

	result := &BenchmarkResult{
		TestName:    fmt.Sprintf("%s_%d_%d_%s", indexTypeToString(indexType), datasetSize, dimension, metricToString(metric)),
		IndexType:   indexTypeToString(indexType),
		DatasetSize: datasetSize,
		Dimension:   dimension,
		QuerySize:   len(queryVectors),
		Metric:      metricToString(metric),
		Timestamp:   time.Now(),
	}

	// Run multiple iterations for stability
	var insertRates, searchLatencies, searchThroughputs []float64
	var memoryUsages []int64

	for run := 0; run < suite.config.NumRuns+suite.config.WarmupRuns; run++ {
		isWarmup := run < suite.config.WarmupRuns

		if !isWarmup {
			fmt.Printf("  üìà Run %d/%d... ", run-suite.config.WarmupRuns+1, suite.config.NumRuns)
		}

		// Create fresh index for each run
		idx, err := suite.createIndex(indexType, config)
		if err != nil {
			return nil, fmt.Errorf("failed to create index: %w", err)
		}

		// Benchmark insertion
		insertRate, err := suite.benchmarkInsertion(idx, testVectors)
		if err != nil {
			return nil, fmt.Errorf("insertion benchmark failed: %w", err)
		}

		// Benchmark search
		searchLatency, searchThroughput, err := suite.benchmarkSearch(idx, queryVectors, 10)
		if err != nil {
			return nil, fmt.Errorf("search benchmark failed: %w", err)
		}

		// Measure memory usage
		memoryUsage := suite.measureMemoryUsage()

		// Record results (skip warmup runs)
		if !isWarmup {
			insertRates = append(insertRates, insertRate)
			searchLatencies = append(searchLatencies, searchLatency)
			searchThroughputs = append(searchThroughputs, searchThroughput)
			memoryUsages = append(memoryUsages, memoryUsage)
			fmt.Printf("‚úì Insert: %.0f v/s, Search: %.2fms, Memory: %d MB\n",
				insertRate, searchLatency, memoryUsage/(1024*1024))
		}

		// Clean up
		idx.Close()
		runtime.GC() // Force garbage collection between runs
	}

	// Calculate averages
	result.InsertRate = average(insertRates)
	result.SearchLatency = average(searchLatencies)
	result.SearchThroughput = average(searchThroughputs)
	result.MemoryUsed = averageInt64(memoryUsages)

	// Calculate accuracy metrics using ground truth
	groundTruthResults, groundTruthLatency := suite.calculateGroundTruth(queryVectors, testVectors, 100)
	result.GroundTruthLatency = groundTruthLatency
	result.SpeedupFactor = groundTruthLatency / result.SearchLatency

	// Calculate recall
	idx, _ := suite.createIndex(indexType, config)
	for _, vector := range testVectors {
		idx.Add(&index.Vector{
			ID:   fmt.Sprintf("vec_%d", len(vector)),
			Data: vector,
		})
	}

	recall10, recall100 := suite.calculateRecall(idx, queryVectors, groundTruthResults, 10, 100)
	result.Recall10 = recall10
	result.Recall100 = recall100

	idx.Close()

	return result, nil
}

// createIndex creates an index of the specified type
func (suite *InternalBenchmarkSuite) createIndex(indexType index.IndexType, config *index.Config) (IndexInterface, error) {
	switch indexType {
	case index.IndexTypeHNSW:
		return index.NewConcurrentHNSWIndex(config)
	default:
		return nil, fmt.Errorf("unsupported index type: %v", indexType)
	}
}

// IndexInterface defines the common interface for all index types
type IndexInterface interface {
	Add(vector *index.Vector) error
	Search(query []float32, k int) ([]*index.SearchResult, error)
	Close() error
}

// benchmarkInsertion measures insertion performance
func (suite *InternalBenchmarkSuite) benchmarkInsertion(idx IndexInterface, vectors [][]float32) (float64, error) {
	startTime := time.Now()

	for i, vector := range vectors {
		vec := &index.Vector{
			ID:   fmt.Sprintf("vec_%d", i),
			Data: vector,
		}
		if err := idx.Add(vec); err != nil {
			return 0, fmt.Errorf("failed to add vector %d: %w", i, err)
		}
	}

	duration := time.Since(startTime)
	rate := float64(len(vectors)) / duration.Seconds()

	return rate, nil
}

// benchmarkSearch measures search performance
func (suite *InternalBenchmarkSuite) benchmarkSearch(idx IndexInterface, queries [][]float32, k int) (float64, float64, error) {
	// Warm up the index
	for i := 0; i < 10; i++ {
		_, _ = idx.Search(queries[0], k)
	}

	startTime := time.Now()

	for _, query := range queries {
		_, err := idx.Search(query, k)
		if err != nil {
			return 0, 0, fmt.Errorf("search failed: %w", err)
		}
	}

	duration := time.Since(startTime)
	avgLatency := float64(duration.Nanoseconds()) / float64(len(queries)) / 1e6 // Convert to milliseconds
	throughput := float64(len(queries)) / duration.Seconds()

	return avgLatency, throughput, nil
}

// measureMemoryUsage measures current memory usage
func (suite *InternalBenchmarkSuite) measureMemoryUsage() int64 {
	var m runtime.MemStats
	runtime.ReadMemStats(&m)
	return int64(m.Alloc)
}

// generateTestVectors generates random test vectors
func (suite *InternalBenchmarkSuite) generateTestVectors(count, dimension int) [][]float32 {
	vectors := make([][]float32, count)

	for i := 0; i < count; i++ {
		vector := make([]float32, dimension)
		for j := 0; j < dimension; j++ {
			vector[j] = float32(i*dimension+j) / float32(count*dimension) // Deterministic for reproducibility
		}
		// Normalize vector
		var norm float32
		for _, v := range vector {
			norm += v * v
		}
		norm = float32(1.0 / (norm + 1e-10)) // Avoid division by zero
		for j := range vector {
			vector[j] *= norm
		}
		vectors[i] = vector
	}

	return vectors
}

// calculateGroundTruth calculates ground truth results using exhaustive search
func (suite *InternalBenchmarkSuite) calculateGroundTruth(queries, corpus [][]float32, k int) ([][]int, float64) {
	results := make([][]int, len(queries))

	startTime := time.Now()

	for i, query := range queries {
		distances := make([]struct {
			index    int
			distance float32
		}, len(corpus))

		// Calculate distances to all vectors
		for j, vector := range corpus {
			distance := index.OptimizedCosineDistance(query, vector)
			distances[j] = struct {
				index    int
				distance float32
			}{j, distance}
		}

		// Sort by distance
		for a := 0; a < len(distances)-1; a++ {
			for b := a + 1; b < len(distances); b++ {
				if distances[a].distance > distances[b].distance {
					distances[a], distances[b] = distances[b], distances[a]
				}
			}
		}

		// Take top k
		topK := make([]int, min(k, len(distances)))
		for j := 0; j < len(topK); j++ {
			topK[j] = distances[j].index
		}
		results[i] = topK
	}

	duration := time.Since(startTime)
	avgLatency := float64(duration.Nanoseconds()) / float64(len(queries)) / 1e6 // milliseconds

	return results, avgLatency
}

// calculateRecall calculates recall metrics
func (suite *InternalBenchmarkSuite) calculateRecall(idx IndexInterface, queries [][]float32, groundTruth [][]int, k1, k2 int) (float64, float64) {
	var recall10Sum, recall100Sum float64

	for i, query := range queries {
		results, err := idx.Search(query, max(k1, k2))
		if err != nil {
			continue
		}

		// Convert results to indices
		resultIndices := make(map[string]bool)
		for _, result := range results {
			resultIndices[result.ID] = true
		}

		// Calculate recall@k1
		if len(groundTruth[i]) >= k1 {
			hits := 0
			for j := 0; j < k1 && j < len(groundTruth[i]); j++ {
				if resultIndices[fmt.Sprintf("vec_%d", groundTruth[i][j])] {
					hits++
				}
			}
			recall10Sum += float64(hits) / float64(k1)
		}

		// Calculate recall@k2
		if len(groundTruth[i]) >= k2 {
			hits := 0
			for j := 0; j < k2 && j < len(groundTruth[i]); j++ {
				if resultIndices[fmt.Sprintf("vec_%d", groundTruth[i][j])] {
					hits++
				}
			}
			recall100Sum += float64(hits) / float64(k2)
		}
	}

	return recall10Sum / float64(len(queries)), recall100Sum / float64(len(queries))
}

// generateReport creates a comprehensive benchmark report
func (suite *InternalBenchmarkSuite) generateReport() error {
	fmt.Println("\nüìã Generating Comprehensive Benchmark Report...")

	// Generate JSON report
	jsonReport, err := json.MarshalIndent(suite.results, "", "  ")
	if err != nil {
		return fmt.Errorf("failed to marshal JSON report: %w", err)
	}

	jsonPath := filepath.Join(suite.config.ProfileOutputDir, "benchmark_results.json")
	if err := os.WriteFile(jsonPath, jsonReport, 0644); err != nil {
		return fmt.Errorf("failed to write JSON report: %w", err)
	}

	// Generate markdown report
	markdownReport := suite.generateMarkdownReport()
	markdownPath := filepath.Join(suite.config.ProfileOutputDir, "benchmark_report.md")
	if err := os.WriteFile(markdownPath, []byte(markdownReport), 0644); err != nil {
		return fmt.Errorf("failed to write markdown report: %w", err)
	}

	// Generate performance comparison
	comparisonReport := suite.generatePerformanceComparison()
	comparisonPath := filepath.Join(suite.config.ProfileOutputDir, "performance_comparison.md")
	if err := os.WriteFile(comparisonPath, []byte(comparisonReport), 0644); err != nil {
		return fmt.Errorf("failed to write comparison report: %w", err)
	}

	fmt.Printf("üìÅ Reports generated in: %s\n", suite.config.ProfileOutputDir)
	fmt.Printf("   ‚Ä¢ benchmark_results.json - Raw JSON data\n")
	fmt.Printf("   ‚Ä¢ benchmark_report.md - Formatted report\n")
	fmt.Printf("   ‚Ä¢ performance_comparison.md - Performance analysis\n")

	return nil
}

// generateMarkdownReport creates a markdown formatted report
func (suite *InternalBenchmarkSuite) generateMarkdownReport() string {
	report := fmt.Sprintf(`# GoVecDB Benchmark Report

**Generated:** %s  
**System:** %s %s  
**Go Version:** %s  
**CPU Cores:** %d  

## Executive Summary

`, time.Now().Format("2006-01-02 15:04:05"), runtime.GOOS, runtime.GOARCH, runtime.Version(), runtime.NumCPU())

	// Calculate summary statistics
	var totalInsertRate, totalSearchLatency, totalThroughput float64
	var maxInsertRate, minSearchLatency, maxThroughput float64
	count := len(suite.results)

	for i, result := range suite.results {
		totalInsertRate += result.InsertRate
		totalSearchLatency += result.SearchLatency
		totalThroughput += result.SearchThroughput

		if i == 0 || result.InsertRate > maxInsertRate {
			maxInsertRate = result.InsertRate
		}
		if i == 0 || result.SearchLatency < minSearchLatency {
			minSearchLatency = result.SearchLatency
		}
		if i == 0 || result.SearchThroughput > maxThroughput {
			maxThroughput = result.SearchThroughput
		}
	}

	report += fmt.Sprintf(`**Average Insert Rate:** %.0f vectors/sec  
**Average Search Latency:** %.2f ms  
**Average Search Throughput:** %.0f queries/sec  

**Peak Insert Rate:** %.0f vectors/sec  
**Best Search Latency:** %.2f ms  
**Peak Search Throughput:** %.0f queries/sec  

`, totalInsertRate/float64(count), totalSearchLatency/float64(count), totalThroughput/float64(count),
		maxInsertRate, minSearchLatency, maxThroughput)

	// Detailed results table
	report += `## Detailed Results

| Test | Dataset | Dim | Metric | Insert Rate | Search Latency | Throughput | Recall@10 | Memory (MB) |
|------|---------|-----|---------|-------------|----------------|------------|-----------|-------------|
`

	for _, result := range suite.results {
		report += fmt.Sprintf("| %s | %d | %d | %s | %.0f v/s | %.2f ms | %.0f q/s | %.2f%% | %.1f |\n",
			result.IndexType, result.DatasetSize, result.Dimension, result.Metric,
			result.InsertRate, result.SearchLatency, result.SearchThroughput,
			result.Recall10*100, float64(result.MemoryUsed)/(1024*1024))
	}

	return report
}

// generatePerformanceComparison creates a performance comparison analysis
func (suite *InternalBenchmarkSuite) generatePerformanceComparison() string {
	comparison := fmt.Sprintf(`# GoVecDB Performance Analysis

**Generated:** %s  

## Performance Comparison with External Systems

Based on our previous external benchmarks:

| System | Insert Rate (v/s) | Search Latency (ms) | Search Throughput (q/s) |
|--------|------------------|---------------------|-------------------------|
| ChromaDB | 5,633 | 1.01 | 994 |
| Qdrant | 2,449 | 13.74 | 73 |
| Weaviate | 472 | 45.60 | 22 |
| **GoVecDB (Optimized)** | **%.0f** | **%.2f** | **%.0f** |

`, time.Now().Format("2006-01-02 15:04:05"),
		suite.getBestInsertRate(), suite.getBestSearchLatency(), suite.getBestThroughput())

	// Performance analysis
	bestInsert := suite.getBestInsertRate()
	bestLatency := suite.getBestSearchLatency()
	bestThroughput := suite.getBestThroughput()

	comparison += fmt.Sprintf(`## Performance Analysis

### Insert Performance
- **GoVecDB Optimized:** %.0f vectors/sec
- **vs ChromaDB:** %.1fx %s
- **vs Qdrant:** %.1fx %s
- **vs Weaviate:** %.1fx %s

### Search Latency
- **GoVecDB Optimized:** %.2f ms
- **vs ChromaDB:** %.1fx %s
- **vs Qdrant:** %.1fx %s  
- **vs Weaviate:** %.1fx %s

### Search Throughput
- **GoVecDB Optimized:** %.0f queries/sec
- **vs ChromaDB:** %.1fx %s
- **vs Qdrant:** %.1fx %s
- **vs Weaviate:** %.1fx %s

`,
		bestInsert,
		bestInsert/5633, compareString(bestInsert, 5633),
		bestInsert/2449, compareString(bestInsert, 2449),
		bestInsert/472, compareString(bestInsert, 472),

		bestLatency,
		bestLatency/1.01, compareStringLatency(bestLatency, 1.01),
		bestLatency/13.74, compareStringLatency(bestLatency, 13.74),
		bestLatency/45.60, compareStringLatency(bestLatency, 45.60),

		bestThroughput,
		bestThroughput/994, compareString(bestThroughput, 994),
		bestThroughput/73, compareString(bestThroughput, 73),
		bestThroughput/22, compareString(bestThroughput, 22))

	return comparison
}

// Utility functions

func (suite *InternalBenchmarkSuite) getBestInsertRate() float64 {
	best := 0.0
	for _, result := range suite.results {
		if result.InsertRate > best {
			best = result.InsertRate
		}
	}
	return best
}

func (suite *InternalBenchmarkSuite) getBestSearchLatency() float64 {
	best := 1e9
	for _, result := range suite.results {
		if result.SearchLatency < best {
			best = result.SearchLatency
		}
	}
	return best
}

func (suite *InternalBenchmarkSuite) getBestThroughput() float64 {
	best := 0.0
	for _, result := range suite.results {
		if result.SearchThroughput > best {
			best = result.SearchThroughput
		}
	}
	return best
}

func (suite *InternalBenchmarkSuite) printTestResult(result *BenchmarkResult) {
	fmt.Printf("  ‚úÖ Results: Insert %.0f v/s | Search %.2fms | Throughput %.0f q/s | Recall@10 %.1f%%\n",
		result.InsertRate, result.SearchLatency, result.SearchThroughput, result.Recall10*100)
}

// Helper functions

func indexTypeToString(indexType index.IndexType) string {
	switch indexType {
	case index.IndexTypeHNSW:
		return "HNSW"
	case index.IndexTypeIVF:
		return "IVF"
	case index.IndexTypeLSH:
		return "LSH"
	case index.IndexTypePQ:
		return "PQ"
	case index.IndexTypeFlat:
		return "Flat"
	default:
		return "Unknown"
	}
}

func metricToString(metric index.DistanceMetric) string {
	switch metric {
	case index.Cosine:
		return "Cosine"
	case index.Euclidean:
		return "Euclidean"
	case index.Manhattan:
		return "Manhattan"
	case index.DotProduct:
		return "DotProduct"
	default:
		return "Unknown"
	}
}

func average(values []float64) float64 {
	if len(values) == 0 {
		return 0
	}
	sum := 0.0
	for _, v := range values {
		sum += v
	}
	return sum / float64(len(values))
}

func averageInt64(values []int64) int64 {
	if len(values) == 0 {
		return 0
	}
	sum := int64(0)
	for _, v := range values {
		sum += v
	}
	return sum / int64(len(values))
}

func min(a, b int) int {
	if a < b {
		return a
	}
	return b
}

func max(a, b int) int {
	if a > b {
		return a
	}
	return b
}

func compareString(a, b float64) string {
	if a > b {
		return "faster"
	}
	return "slower"
}

func compareStringLatency(a, b float64) string {
	if a < b {
		return "faster"
	}
	return "slower"
}

// Main function to run the benchmark suite
func main() {
	fmt.Println("üöÄ GoVecDB Internal Benchmark Suite")
	fmt.Println("====================================")

	config := &BenchmarkConfig{
		DatasetSizes:          []int{1000, 5000, 10000},
		Dimensions:            []int{128, 256, 512},
		QuerySizes:            []int{100},
		IndexTypes:            []index.IndexType{index.IndexTypeHNSW},
		MetricTypes:           []index.DistanceMetric{index.Cosine, index.Euclidean},
		NumRuns:               3,
		WarmupRuns:            1,
		TimeoutSec:            300,
		EnableMemoryProfiling: true,
		ProfileOutputDir:      "./internal-benchmarks",
	}

	suite := NewInternalBenchmarkSuite(config)

	if err := suite.RunComprehensiveBenchmarks(); err != nil {
		log.Fatalf("Benchmark suite failed: %v", err)
	}

	fmt.Println("\nüéâ All benchmarks completed successfully!")
	fmt.Println("üìä Check the generated reports for detailed analysis.")
}
