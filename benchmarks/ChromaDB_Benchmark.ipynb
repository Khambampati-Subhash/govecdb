{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3b50996",
   "metadata": {},
   "source": [
    "# 🚀 ChromaDB Performance Benchmark\n",
    "\n",
    "Comprehensive performance testing for ChromaDB across multiple dimensions and operations.\n",
    "\n",
    "This notebook will test:\n",
    "- Vector dimensions: 2 to 2048\n",
    "- Vector counts: 1K to 100K\n",
    "- All CRUD operations\n",
    "- Search performance and quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2aff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Install dependencies\n",
    "!pip install -q chromadb numpy pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397b6a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📥 Import libraries\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"✅ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae7d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📋 Benchmark Configuration\n",
    "\n",
    "# You can modify these settings:\n",
    "DIMENSIONS = [2, 8, 16, 32, 64, 128, 256, 384, 512, 768, 1024, 1536, 2048]\n",
    "VECTOR_COUNTS = [1000, 5000, 10000, 25000, 50000, 100000]\n",
    "SEARCH_K = 10\n",
    "NUM_SEARCHES = 100\n",
    "\n",
    "print(\"📊 Test Configuration:\")\n",
    "print(f\"   Dimensions: {DIMENSIONS}\")\n",
    "print(f\"   Vector Counts: {VECTOR_COUNTS}\")\n",
    "print(f\"   Search K: {SEARCH_K}\")\n",
    "print(f\"   Number of Searches: {NUM_SEARCHES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545a61c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste the entire ChromaDB benchmark code here\n",
    "# Copy from chromadb_benchmark.py (the class and functions)\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    dimension: int\n",
    "    num_vectors: int\n",
    "    operation: str\n",
    "    total_time: float\n",
    "    avg_time: float\n",
    "    min_time: float\n",
    "    max_time: float\n",
    "    throughput: float\n",
    "    recall: float = 0.0\n",
    "    search_quality: float = 0.0\n",
    "\n",
    "class ChromaDBBenchmark:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize ChromaDB client\"\"\"\n",
    "        self.client = chromadb.Client(Settings(\n",
    "            chroma_db_impl=\"duckdb+parquet\",\n",
    "            persist_directory=None,\n",
    "            anonymized_telemetry=False\n",
    "        ))\n",
    "        \n",
    "    def generate_vectors(self, count: int, dim: int, seed: int = 42) -> Tuple[np.ndarray, List[str], List[Dict]]:\n",
    "        \"\"\"Generate normalized random vectors with metadata\"\"\"\n",
    "        np.random.seed(seed)\n",
    "        vectors = np.random.randn(count, dim).astype(np.float32)\n",
    "        norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "        vectors = vectors / (norms + 1e-8)\n",
    "        ids = [f\"vec_{i}\" for i in range(count)]\n",
    "        categories = [\"tech\", \"science\", \"arts\", \"sports\", \"news\"]\n",
    "        metadatas = [\n",
    "            {\n",
    "                \"index\": i,\n",
    "                \"category\": categories[i % len(categories)],\n",
    "                \"score\": float(np.random.random()),\n",
    "                \"group\": i % 10,\n",
    "            }\n",
    "            for i in range(count)\n",
    "        ]\n",
    "        return vectors, ids, metadatas\n",
    "    \n",
    "    # Add all other methods from chromadb_benchmark.py here...\n",
    "    # (For brevity, include the full class definition)\n",
    "\n",
    "print(\"✅ Benchmark class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09349dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 Run the benchmark!\n",
    "\n",
    "print(\"🚀 Starting ChromaDB Comprehensive Performance Benchmark\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "benchmark = ChromaDBBenchmark()\n",
    "all_results = []\n",
    "\n",
    "# Run benchmarks\n",
    "for dim in DIMENSIONS:\n",
    "    print(f\"\\n📊 Testing Dimension: {dim}\")\n",
    "    print(\"─\" * 60)\n",
    "    \n",
    "    for num_vectors in VECTOR_COUNTS:\n",
    "        # Skip very large combinations\n",
    "        if dim >= 1536 and num_vectors > 25000:\n",
    "            continue\n",
    "        if dim >= 1024 and num_vectors > 50000:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n  📦 Vector Count: {num_vectors}\")\n",
    "        \n",
    "        results = benchmark.run_benchmark(dim, num_vectors, SEARCH_K, NUM_SEARCHES)\n",
    "        all_results.extend(results)\n",
    "\n",
    "print(\"\\n✅ All benchmarks complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ce50b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Display Summary\n",
    "\n",
    "df = pd.DataFrame([asdict(r) for r in all_results])\n",
    "\n",
    "print(\"\\n🏆 FINAL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "for operation in df['operation'].unique():\n",
    "    op_data = df[df['operation'] == operation]\n",
    "    \n",
    "    print(f\"\\n📊 {operation}\")\n",
    "    print(\"─\" * 80)\n",
    "    \n",
    "    avg_ms = op_data['avg_time'].mean() * 1000\n",
    "    min_ms = op_data['avg_time'].min() * 1000\n",
    "    max_ms = op_data['avg_time'].max() * 1000\n",
    "    \n",
    "    print(f\"  Average: {avg_ms:.3f}ms | Min: {min_ms:.3f}ms | Max: {max_ms:.3f}ms\")\n",
    "    \n",
    "    if op_data['throughput'].mean() > 0:\n",
    "        print(f\"  Throughput: {op_data['throughput'].mean():.0f} ops/sec (avg)\")\n",
    "    \n",
    "    if operation == 'exact_search' and op_data['recall'].mean() > 0:\n",
    "        print(f\"  Recall: {op_data['recall'].mean()*100:.2f}%\")\n",
    "    \n",
    "    if 'search_k' in operation and op_data['search_quality'].mean() > 0:\n",
    "        print(f\"  Search Quality: {op_data['search_quality'].mean()*100:.2f}%\")\n",
    "\n",
    "print(\"\\n✅ Benchmark Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0394259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💾 Save Results\n",
    "\n",
    "results_dict = [asdict(r) for r in all_results]\n",
    "\n",
    "with open('chromadb_benchmark_results.json', 'w') as f:\n",
    "    json.dump(results_dict, f, indent=2)\n",
    "\n",
    "df.to_csv('chromadb_benchmark_results.csv', index=False)\n",
    "\n",
    "print(\"💾 Results saved:\")\n",
    "print(\"   - chromadb_benchmark_results.json\")\n",
    "print(\"   - chromadb_benchmark_results.csv\")\n",
    "\n",
    "# Download files\n",
    "from google.colab import files\n",
    "files.download('chromadb_benchmark_results.json')\n",
    "files.download('chromadb_benchmark_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c5b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📈 Visualizations\n",
    "\n",
    "# Search latency by dimension\n",
    "search_data = df[df['operation'] == 'search_k10']\n",
    "if len(search_data) > 0:\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    for count in search_data['num_vectors'].unique():\n",
    "        data = search_data[search_data['num_vectors'] == count]\n",
    "        plt.plot(data['dimension'], data['avg_time'] * 1000, marker='o', label=f\"{count} vectors\")\n",
    "    plt.xlabel('Dimension')\n",
    "    plt.ylabel('Latency (ms)')\n",
    "    plt.title('Search Latency vs Dimension')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    for dim in [128, 384, 768, 1024]:\n",
    "        data = search_data[search_data['dimension'] == dim]\n",
    "        if len(data) > 0:\n",
    "            plt.plot(data['num_vectors'], data['avg_time'] * 1000, marker='o', label=f\"{dim}D\")\n",
    "    plt.xlabel('Number of Vectors')\n",
    "    plt.ylabel('Latency (ms)')\n",
    "    plt.title('Search Latency vs Vector Count')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('chromadb_search_performance.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "# Throughput comparison\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "ops = ['batch_insert', 'search_k10']\n",
    "for i, op in enumerate(ops, 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    op_data = df[df['operation'] == op]\n",
    "    if len(op_data) > 0:\n",
    "        grouped = op_data.groupby('dimension')['throughput'].mean()\n",
    "        plt.bar(range(len(grouped)), grouped.values)\n",
    "        plt.xticks(range(len(grouped)), grouped.index, rotation=45)\n",
    "        plt.xlabel('Dimension')\n",
    "        plt.ylabel('Throughput (ops/sec)')\n",
    "        plt.title(f'{op.replace(\"_\", \" \").title()} Throughput')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chromadb_throughput.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"📊 Visualizations saved:\")\n",
    "print(\"   - chromadb_search_performance.png\")\n",
    "print(\"   - chromadb_throughput.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4967a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📋 Summary Table\n",
    "\n",
    "summary = df.groupby('operation').agg({\n",
    "    'avg_time': ['mean', 'min', 'max'],\n",
    "    'throughput': 'mean',\n",
    "    'recall': 'mean',\n",
    "    'search_quality': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "# Convert times to milliseconds\n",
    "summary['avg_time'] = summary['avg_time'] * 1000\n",
    "\n",
    "print(\"\\n📊 Operation Summary\")\n",
    "print(summary)\n",
    "\n",
    "summary.to_csv('chromadb_summary.csv')\n",
    "print(\"\\n💾 Summary saved to: chromadb_summary.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
